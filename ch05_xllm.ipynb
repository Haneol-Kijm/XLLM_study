{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency 문제 해결법\n",
    "해당 방법은 제 맥북 맥os 기준이므로, 윈도우나 다른 환경에서는 해결 방법이 조금 다를 수 있습니다.\n",
    "1. torchtext.data는 0.10.0 이후로 outdated 됐으므로 그 이하 버전을 설치해야합니다. 저는 검색해서 문서가 나오는 0.8.0을 선택했습니다.\n",
    "2. torchtext는 최신 파이토치와 호환이 안되므로 버전에 맞춰서 설치해야합니다. \n",
    "torchtext와 호환되는 파이토치 버전은 1.7, 파이썬 버전은 3.6<=python<=3.8이므로 이에 맞춰 환경을 설치했습니다.\n",
    "참고 링크: [version compatibility](https://pypi.org/project/torchtext/)\n",
    "3. 그런데 또 파이썬 3.8에선 무슨 이유에선지 파이토치 1.7 버전이 깔리지 않아 파이썬을 3.7 버전으로 내리고 설치에 성공했습니다.\n",
    "판단 근거: 분명 [파이썬 3.9 이상에서 설치가 안된다](https://stackoverflow.com/questions/75823700/i-am-unable-to-install-pytorch-1-7-1-using-pip-conda)고 하는데 3.8에서도 안되는 것으로 추론하여 버전을 낮췄습니다.\n",
    "4. 파이썬 3.7은 환경 생성을 바로 할 수 없으므로 [다음 링크](https://velog.io/@hotsun1508/Error-%ED%95%B4%EA%B2%B0-M2%EC%97%90-%EC%95%84%EB%82%98%EC%BD%98%EB%8B%A4-python-3.7-%EA%B0%80%EC%83%81%ED%99%98%EA%B2%BD-%EC%83%9D%EC%84%B1)를 참고했습니다.\n",
    "5. 마지막으로 conda-forge 채널을 통해 spacy를 설치했습니다. en_core_web_sm 모델도 같이 받아야합니다.\n",
    "그런데 다음 오류를 겪었습니다:  \n",
    "OSError: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n",
    "spacy 3.x 이상의 버전 문제라고 판단하여, 버전도 2.1.0으로 pip를 통해 낮춰서 설치했습니다.\n",
    "이후 동일하게 en 모델을 찾을 수 없다는 오류가 떴습니다. 그래서 언어 모델도 en_core_web_sm이 아니라, 다음 명령어로 'en' 모델을 설치하여 최종적으로 문제를 해결했습니다.  \n",
    "```python -m spacy download en```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from torchtext import *\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 비맥북의 경우\n",
    "# dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# 맥북의 경우 pytorch 1.7.0에서 gpu 연산을 지원하지 않습니다.\n",
    "dev = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/opt/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/opt/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/opt/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# set up fields\n",
    "TEXT = data.Field(lower=True, include_lengths=True, \\\n",
    "batch_first=False, tokenize='spacy')\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# make splits for data\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d') \\\n",
    "# use 'glove.42B.300d' for greater accuracy or 'glove.6B.100d' for greater speed\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), \\\n",
    "batch_sizes=(128,1024), device=dev, sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(torch.nn.Module):\n",
    "    def __init__(self, embedding_size = 100, hidden_size = 512, num_layers = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up an embedding layer with the right dimensions, and copy the weights from the pretrained glove embeddings\n",
    "        vocab = TEXT.vocab\n",
    "        self.embed = torch.nn.Embedding(len(vocab), embedding_size)\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        # Set up a standard PyTorch RNN sections with the right dimensions and a variable number of layers\n",
    "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        # Add a two layer classification head with the right dimensions. The final layer must output a single number\n",
    "        self.classificationLayer1 = torch.nn.Linear(hidden_size,10)\n",
    "        self.classificationLayer2 = torch.nn.Linear(10,1)\n",
    "\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "\n",
    "        embed_input = self.embed(input)\n",
    "        packed_emb = torch.nn.utils.rnn.pack_padded_sequence(embed_input, \\\n",
    "        lengths, batch_first=False)\n",
    "\n",
    "        output, hidden = self.rnn(packed_emb)\n",
    "        hidden = hidden[-1]\n",
    "        x = hidden.squeeze(0)\n",
    "        x = self.classificationLayer1(x)\n",
    "        x = self.classificationLayer2(x)\n",
    "\n",
    "        logits = x.view(-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_classifier(\n",
       "  (embed): Embedding(101980, 100)\n",
       "  (rnn): RNN(100, 256)\n",
       "  (classificationLayer1): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (classificationLayer2): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN_classifier(hidden_size=256, num_layers=1)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    (x,x_len) = batch.text\n",
    "    pred = model(x,x_len)\n",
    "    print(pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.binary_cross_entropy_with_logits\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, test_data):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(test_data):\n",
    "            text, text_lengths = batch_data.text\n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            total += batch_data.label.size(0)\n",
    "            correct += (predicted_labels == batch_data.label.long()).sum()\n",
    "        return correct.float()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e232b2715e443dd98cfccdd2a188c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408a033ba6c64ba68fa24858bc569dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66336\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 주피터 노트북의 경우 ipywidgets 필요(conda, pip)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_iter):\n",
    "        (x,x_lengths)=batch.text\n",
    "        pred = model(x,x_lengths)\n",
    "\n",
    "        actual=batch.label.float()\n",
    "        loss = loss_func(pred,actual)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    if (epoch==5):\n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = 3e-3\n",
    "\n",
    "    print(\"Accuracy: \" + str(get_metrics(model, test_iter).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "\n",
    "    tensor = torch.LongTensor(indexed).to(dev)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5359222888946533"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"\"\"I like that Far From Home is trying something new and that its\n",
    "humor  feels more real than the ironic cracks in most superhero movies.\n",
    "I just wish its good pieces all came together more satisfyingly.\"\"\"\n",
    "\n",
    "print('Probability positive:')\n",
    "predict_sentiment(model, review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
